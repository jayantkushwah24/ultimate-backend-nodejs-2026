# Processing at Scale & Fundamentals of Threads â€” SDE-3 Deep Dive

---

## ğŸ”· 1. How Programs Execute on Our Machines

### Execution Pipeline

Program (binary / script)
â†“
Process created by OS
â†“
Threads scheduled
â†“
CPU executes instructions


### Key Concepts

- **Program** â†’ static file
- **Process** â†’ running instance with memory + resources
- **Thread** â†’ execution path inside process
- **CPU** â†’ executes machine instructions
- **Kernel Scheduler** â†’ decides which thread runs

---

## ğŸ”· 2. Step-by-Step: How Software Actually Runs

### Detailed Flow

User runs program
â†“
Kernel creates process
â†“
Virtual memory allocated
â†“
Executable mapped into memory
â†“
CPU starts execution (user mode)
â†“
System calls when OS services needed
â†“
Interrupts + scheduler manage CPU time


### User Mode vs Kernel Mode

| Mode | Access |
|------|--------|
| User | Restricted |
| Kernel | Full hardware access |

**Interview tip:** Always mention privilege rings.

---

## ğŸ”· 3. How Computers Run Many Tasks

### Concurrency vs Parallelism

Single Core (Concurrency)

Time â†’
[T1][T2][T3][T1][T2]

Multi-Core (Parallelism)

Core1: [T1][T1][T1]
Core2: [T2][T2][T2]
Core3: [T3][T3][T3]


### Mechanisms Enabling Scale

- Timer interrupts
- Preemptive scheduling
- DMA (Direct Memory Access)
- Async I/O
- Thread pools

---

## ğŸ”· 4. Context Switching â€” Deep Dive

### Definition

Context switch = saving one threadâ€™s state and restoring anotherâ€™s.

---

### What Gets Saved

Registers
Program Counter
Stack Pointer
CPU flags
SIMD/FPU state
Kernel metadata


---

### Context Switch Flow

Timer interrupt fires
â†“
CPU enters kernel mode
â†“
Kernel saves current thread state
â†“
Scheduler picks next thread
â†“
Kernel restores next thread state
â†“
Return to user mode


---

### Performance Costs

**Major costs:**

- Register save/restore
- Cache cold start
- TLB invalidation
- Kernel overhead

âš ï¸ Frequent context switching = throughput killer.

---

### Process vs Thread Switch

| Type | Cost |
|------|------|
| Thread switch | Medium |
| Process switch | High (page table change) |

---

## ğŸ”· 5. Client-Server Architecture

### Basic Model

Client â†’ Request â†’ Server
Client â† Response â† Server


---

### Definitions

**Client**
- Initiates communication
- Example: browser, mobile app

**Server**
- Processes requests
- Provides resources/services

---

### Scaled Architecture

Clients
â†“
Load Balancer
â†“
App Servers
â†“
Cache / DB / Queue


---

### Key Properties

- Stateless servers scale better
- Idempotent APIs enable retries
- Backpressure prevents overload
- Horizontal scaling preferred

---

## ğŸ”· 6. Introduction to Threads

### Definition

A thread is the smallest unit of execution inside a process.

---

### Process vs Thread

| Feature | Process | Thread |
|--------|--------|--------|
| Memory | Separate | Shared |
| Creation cost | High | Lower |
| Communication | IPC | Shared memory |
| Isolation | Strong | Weak |

---

### Thread Lifecycle

NEW â†’ RUNNABLE â†’ RUNNING â†’ BLOCKED â†’ TERMINATED


---

### Types of Threads

- Kernel threads
- User (green) threads
- M:N runtime threads (Go model)

---

## ğŸ”· 7. Synchronization Primitives

### Mutex

lock()
critical section
unlock()


Use when exclusive access required.

---

### Semaphore

Controls access to **N resources**.

---

### Condition Variable

Used for wait/notify patterns.

---

### Atomics & CAS

Important for lock-free systems.

CAS(ptr, expected, new)


---

### Common Failures

**Deadlock conditions:**

1. Mutual exclusion  
2. Hold and wait  
3. No preemption  
4. Circular wait  

Break any one â†’ no deadlock.

---

## ğŸ”· 8. Thread Pools & Event Models

### Thread Pool

Task Queue â†’ Worker Threads â†’ Execution


**Benefits**

- Limits concurrency
- Reduces thread creation cost
- Improves CPU utilization

---

### Reactor Pattern (Event Loop)

Event Demultiplexer
â†“
Event Loop
â†“
Handlers


Best for high I/O concurrency.

---

### When to Use What

| Workload | Best Model |
|---------|-----------|
| CPU heavy | Thread pool |
| Massive I/O | Event loop |
| Mixed | Hybrid |

---

## ğŸ”· 9. Memory & Cache Effects (SDE-3 Gold)

### Memory Hierarchy

Registers
â†“
L1 Cache
â†“
L2 Cache
â†“
L3 Cache
â†“
RAM
â†“
Disk


---

### Critical Performance Killers

- Cache misses
- False sharing
- TLB misses
- Page faults
- NUMA remote memory

---

## ğŸ”· 10. Key Laws You Must Know

### Amdahlâ€™s Law

S = 1 / ((1 - p) + p/N)


Limits parallel speedup.

---

### Littleâ€™s Law

L = Î» Ã— W


Used for queue sizing.

---

---

# ğŸ¯ MOCK INTERVIEW QUESTIONS (SDE-3 LEVEL)

---

## â“ Execution & OS

**Q:** What happens when a program starts?

**Model Answer:**  
Kernel creates a process, sets up virtual memory and page tables, maps executable segments, initializes stack/heap, then schedules the main thread for execution in user mode.

---

## â“ Context Switching

**Q:** Why is context switching expensive?

**Model Answer:**  
Because it involves saving/restoring registers, possible TLB flush, cache pollution, and kernel scheduler overhead. Frequent switching reduces CPU cache locality and overall throughput.

---

## â“ Threads vs Processes

**Q:** When would you prefer threads over processes?

**Model Answer:**  
When tasks share memory heavily and require low-latency communication. Processes are preferred when isolation and fault containment matter more.

---

## â“ Client-Server Scaling

**Q:** How do you scale a stateless service?

**Model Answer:**  

- Add load balancer  
- Horizontally scale replicas  
- Externalize session state  
- Add caching  
- Implement backpressure  

---

## â“ Synchronization

**Q:** How do you avoid deadlocks in production systems?

**Model Answer:**

- Global lock ordering  
- Timeouts + retry  
- Try-lock with backoff  
- Reduce lock scope  
- Prefer lock-free structures where safe  

---

## â“ Thread Pool Sizing

**Q:** How many threads should a service have?

**Model Answer:**

CPU-bound:

â‰ˆ number of cores


I/O-bound:

cores Ã— (1 + wait_time / compute_time)


Always validate with load testing.

---

---

# â±ï¸ 60-MINUTE SDE-3 STUDY PLAN

## Minute 0â€“10 â€” Core Mental Model

- Process vs thread
- User vs kernel mode
- Virtual memory basics

âœ… Goal: explain execution pipeline clearly.

---

## Minute 10â€“25 â€” Context Switching Mastery

Study deeply:

- What is saved
- Why expensive
- TLB/cache impact
- Thread vs process switch

ğŸ”¥ This is heavily tested.

---

## Minute 25â€“40 â€” Concurrency Models

Focus on:

- Thread pools
- Event loop (reactor)
- Async I/O
- Backpressure

Be ready to compare tradeoffs.

---

## Minute 40â€“50 â€” Synchronization

Must know cold:

- Mutex
- Semaphore
- CAS
- Deadlock prevention
- Memory visibility

---

## Minute 50â€“60 â€” Scale Thinking

Practice explaining:

- Client-server scaling
- Stateless design
- Queue backpressure
- Littleâ€™s Law usage

---

# ğŸ§  Final SDE-3 Reality Check

If you truly understand this topic, you should be able to:

- Draw full execution path from code â†’ CPU  
- Explain context switch at register level  
- Compare event loop vs thread-per-request  
- Diagnose thread pool saturation  
- Reason about cache/TLB effects  
- Apply Littleâ€™s Law in capacity planning  

If you cannot do these **without notes**, you are not yet at SDE-3 depth.

---

# âœ… Next Recommended Step

If you want to stand out for FAANG-level backend roles, the next deep dives should be:

- Lock-free data structures  
- Futex internals (Linux)  
- epoll vs kqueue vs IOCP  
- NUMA-aware systems  
- Go scheduler / Node libuv internals  
